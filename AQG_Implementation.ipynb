{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AQG Implementation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/himanshututeja1998/Automatic-Question-Generation/blob/master/AQG_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu86QNdFWSgz",
        "colab_type": "code",
        "outputId": "928cf24a-e25d-4cc4-b026-2d49ed64b051",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "#Implementation for the publication https://ieeexplore.ieee.org/document/7732102/\n",
        "# A few enhancements have been done to extract answers and generalize the rules based on syntax.\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "!pip install stanfordcorenlp\n",
        "from stanfordcorenlp import StanfordCoreNLP\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import Tree\n",
        "from nltk import pos_tag\n",
        "from nltk.chunk import RegexpParser\n",
        "from nltk import ne_chunk\n",
        "import itertools\n",
        "import collections\n",
        "import logging\n",
        "#requires Java 1.8 or above\n",
        "#Start  a stanforrrdd CoreNLP server - used stanford-corenlp-full-2018-02-27 for development\n",
        "#java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer   -port 9000 -timeout 150000\n",
        "#import logging\n",
        "import os       #importing os to set environment variable\n",
        "def install_java():\n",
        "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
        "  !java -version       #check java version\n",
        "install_java()\n",
        "nlp = StanfordCoreNLP('http://corenlp.run/',8080)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "Collecting stanfordcorenlp\n",
            "  Downloading https://files.pythonhosted.org/packages/35/cb/0a271890bbe3a77fc1aca2bc3a58b14e11799ea77cb5f7d6fb0a8b4c46fa/stanfordcorenlp-3.9.1.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from stanfordcorenlp) (5.4.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanfordcorenlp) (2.18.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordcorenlp) (2019.3.9)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordcorenlp) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordcorenlp) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordcorenlp) (3.0.4)\n",
            "Installing collected packages: stanfordcorenlp\n",
            "Successfully installed stanfordcorenlp-3.9.1.1\n",
            "openjdk version \"10.0.2\" 2018-07-17\n",
            "OpenJDK Runtime Environment (build 10.0.2+13-Ubuntu-1ubuntu0.18.04.4)\n",
            "OpenJDK 64-Bit Server VM (build 10.0.2+13-Ubuntu-1ubuntu0.18.04.4, mixed mode)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iqDKFpAWShQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Starting New Paper Impl - A Rule based Question Generation Framework to deal with Simple and\n",
        "#Complex Sentences\n",
        "#sentence = \"Barack Obama is the president of The United States of America.\"\n",
        "#sentence = \"The boy went by bus.\"\n",
        "#sentence =  sentence.rstrip().rstrip(\".\")\n",
        "#sentence = \"The contractor will build you a house for $100,000 dollars.\"\n",
        "#sentence = \"The book might cost me $10.\"\n",
        "#sentence=\"The book might cost me $10 from the store.\"\n",
        "#sentence = \"$100,000 builds a house out of sticks.\"\n",
        "#sentence = \"The bill will cost them 500 million dollars in India.\"\n",
        "#sentence = \"His name is Robinson.\"\n",
        "#sentence = \"She will quickly pour the sticky liquid into the green flowery pot.\"\n",
        "#sentence = \"I am going quickly back on Saturday.\"\n",
        "#sentence = \"He wants to become a good doctor.\"\n",
        "#sentence = \"I want to work.\"\n",
        "#sentence = \"He hurriedly left the class in the morning.\"\n",
        "#sentence = \"He is addicted to smoking.\"\n",
        "#sentence = \"He will go by bus.\"\n",
        "#sentence = \"John gave Mary a book.\" #design more rules to catch the essence\n",
        "#sentence = \"He gave him a book.\"\n",
        "#sentence = \"He will buy a book.\"\n",
        "#sentence = \"He gave him a book.\"\n",
        "#sentence = \"John gave Mary a book.\"\n",
        "\n",
        "#print(segments)\n",
        "#dep = nlp.dependency_parse(sentence)\n",
        "#print(dep)\n",
        "#tree.draw()\n",
        "\n",
        "#print(ner)\n",
        "#pos = tree.treeposition_spanning_leaves(0,9)\n",
        "\n",
        "def get_lca_length(location1, location2):\n",
        "    i = 0\n",
        "    while i < len(location1) and i < len(location2) and location1[i] == location2[i]:\n",
        "        i+=1\n",
        "    return i\n",
        "\n",
        "def findLCA(ptree, text1, text2):\n",
        "    leaf_values = ptree.leaves()\n",
        "    leaf_index1 = leaf_values.index(text1)\n",
        "    leaf_index2 = leaf_values.index(text2)\n",
        "\n",
        "    location1 = ptree.leaf_treeposition(leaf_index1)\n",
        "    location2 = ptree.leaf_treeposition(leaf_index2)\n",
        "\n",
        "    #find length of least common ancestor (lca)\n",
        "    lca_len = get_lca_length(location1, location2)\n",
        "    return ptree[location1[:lca_len]]\n",
        "#for ptree in parse_trees :\n",
        "    #ptree.draw()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c36tyRPWShc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_chunks(tagged_segment , grammar):\n",
        "    #print(tagged_segment)\n",
        "    grammar = r\"CHUNK: \" + grammar\n",
        "    #print(grammar)\n",
        "    cp = RegexpParser(grammar)\n",
        "    tree = cp.parse(tagged_segment)\n",
        "    return tree\n",
        "def find_chunk(chunks):\n",
        "    if not isinstance(chunks, nltk.tree.Tree):\n",
        "         return [ [subtree for subtree in chunk.subtrees(filter = lambda t: t.label() in ['CHUNK']) ] for chunk in chunks]   \n",
        "    else :\n",
        "        return [subtree for subtree in chunks.subtrees(filter = lambda t: t.label() in ['CHUNK'])]\n",
        "def is_clause(segment_chunks_tree):\n",
        "    if  not isinstance(chunks, nltk.Tree):\n",
        "         return [ not not chunk for chunk in find_chunk(chunks) ]   \n",
        "    else :\n",
        "        return not not find_chunk(chunks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRDfilFWWSho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#handling Special case :If a segment contains only verb phrase,\n",
        "#the previous segments are also checked for the existence\n",
        "#of any subject phrase related to the verb phrase.\n",
        "\n",
        "def is_only_VP(parse_trees):\n",
        "    return [tree.label() == 'VP' for tree in parse_trees]\n",
        "\n",
        "\n",
        "def find_the_closest_NP_for_VP(parse_trees):\n",
        "    is_VP = is_only_VP(parse_trees)\n",
        "    closest_NP = []\n",
        "    for index,truth_val in enumerate(is_VP):\n",
        "        if not truth_val:\n",
        "            closest_NP.append(None)\n",
        "        else:\n",
        "            found_NP = False\n",
        "            for index_tree in reversed(range(0,index)):\n",
        "                for child in parse_trees[index_tree] :\n",
        "                    if child.label() == 'NP':\n",
        "                        closest_NP.append(child)\n",
        "                        found_NP = True\n",
        "                        break\n",
        "                if found_NP:\n",
        "                    break\n",
        "            if not found_NP :\n",
        "                closest_NP.append(None)\n",
        "    return closest_NP\n",
        "def enrich_VPs(parse_trees):\n",
        "    enrichment_data = find_the_closest_NP_for_VP(parse_trees)\n",
        "    enriched_parse_trees =  []\n",
        "    enrichment_done =[]\n",
        "    for ptree,enrich in zip(parse_trees,enrichment_data):\n",
        "        if enrich :\n",
        "            enriched_parse_trees.append(Tree('S', [enrich.copy(deep=True),ptree]))\n",
        "        else:\n",
        "            enriched_parse_trees.append(ptree)\n",
        "    return enriched_parse_trees , [ not not data for data in enrichment_data ]\n",
        "\n",
        "\n",
        "\n",
        "def find_VP_tree(parse_tree):\n",
        "    for child  in parse_tree :\n",
        "        #print (child.label())\n",
        "        if child.label() == \"VP\":\n",
        "            return child\n",
        "def find_NP_tree(parse_tree):\n",
        "    for child  in parse_tree :\n",
        "        #print (child.label())\n",
        "        if child.label() == \"NP\":\n",
        "            return child\n",
        "\n",
        "def chunk_VP_NP_parts(parse_tree,chunk_tree) :\n",
        "    NP = find_NP_tree(parse_tree).leaves()\n",
        "    VP = find_VP_tree(parse_tree).leaves()\n",
        "    #print(NP,VP)\n",
        "    NP_POS = []\n",
        "    VP_POS = []\n",
        "    #print(chunk_tree)\n",
        "    chunk_pos = chunk_tree.pos()\n",
        "    #print(chunk_pos)\n",
        "    #print(chunk_pos)\n",
        "    for pos in chunk_pos :\n",
        "        if pos[0][0] in NP :\n",
        "            NP.remove(pos[0][0])\n",
        "            NP_POS.append(pos[0])\n",
        "        elif pos[0][0] in VP:\n",
        "            VP.remove(pos[0][0])\n",
        "            VP_POS.append(pos[0])\n",
        "    #print(NP_POS,VP_POS)\n",
        "    return NP_POS,VP_POS\n",
        "\n",
        "def verb_phrase_identification(parse_trees,is_clause,chunks):\n",
        "    verb_phrase = [] \n",
        "    for tree,chunk,is_clause in zip(parse_trees,chunks,is_clause) :\n",
        "        if is_clause :\n",
        "            #print(tree)\n",
        "            #print(type(chunk))\n",
        "            chunk_tree = find_chunk(chunk)[0]\n",
        "            #print(chunk_tree)\n",
        "            #print(tree)\n",
        "            NP_POS,VP_POS = chunk_VP_NP_parts(tree,chunk_tree)\n",
        "            #print(VP_POS)       \n",
        "            if len(VP_POS) > 1 :\n",
        "                verb_phrase.append(VP_POS[0][0])\n",
        "            else :\n",
        "                vp_tag = VP_POS[0][1]\n",
        "                if vp_tag == \"VBD\" :\n",
        "                    verb_phrase.append(\"did\")\n",
        "                elif vp_tag == \"VBP\" or vp_tag == \"VB\" :\n",
        "                    verb_phrase.append(\"do\")\n",
        "                elif vp_tag == \"VBZ\" :\n",
        "                    verb_phrase.append(\"does\")\n",
        "                else :\n",
        "                    verb_phrase.append(None)\n",
        "        else :\n",
        "            verb_phrase.append(None)\n",
        "    return verb_phrase\n",
        "            \n",
        "\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "mUMOy78HWSh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_subj(parse_tree):\n",
        "    for child  in parse_tree :\n",
        "        #print (child.label())\n",
        "        if child.label() == \"NP\":\n",
        "            return child.leaves()\n",
        "    return []\n",
        "def find_VP(parse_tree):\n",
        "    for child  in parse_tree :\n",
        "        #print (child.label())\n",
        "        if child.label() == \"VP\":\n",
        "            return child.leaves()\n",
        "    return []\n",
        "def QSG_Rule_6_1(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases):\n",
        "    QA = []\n",
        "    for chunk,parse_tree,is_cl,ner,tok,verb in zip(chunks , parse_trees , is_clause_val, ner_split,tokens,verb_phrases) :\n",
        "    # Counter example for is_cl - \"A house is for $100,00\"\n",
        "        rule_grammar = \"{<IN>+<\\$>*<CD>+}\"\n",
        "        #print(rule_grammar)\n",
        "        seg_pos = parse_tree.pos()\n",
        "        #print(seg_pos)\n",
        "        rule6_1_chunks = parse_chunks(seg_pos,rule_grammar)\n",
        "        #print(rule6_1_chunks)\n",
        "        prep_chunk = find_chunk(rule6_1_chunks)\n",
        "        #print(prep_chunk)\n",
        "        if prep_chunk :\n",
        "            prep_pos =  [ pos[0] for pos in prep_chunk[0].pos() ]\n",
        "            prep_tokens = [ pos[0][0] for pos in prep_chunk[0].pos() ]\n",
        "            prep_part_tokens = [ p[0] for p in find_chunk(parse_chunks(prep_pos , \"{<IN>+}\" ))[0].leaves()]\n",
        "            #print(prep_part_tokens)\n",
        "            answer_words = [x for x in prep_tokens if x not in prep_part_tokens ] \n",
        "            #print(answer)\n",
        "            subject = find_subj(parse_tree)\n",
        "            VP = find_VP(parse_tree)\n",
        "            rem_verb_phrase =  verb if verb in VP else VP[0][0]\n",
        "            VP = VP[1:]\n",
        "            \n",
        "            #Calc rest of VP for Question Generation\n",
        "            [VP.remove(x) for x in prep_part_tokens + answer_words if x in VP]\n",
        "            #calc rest of the sentence for question generation\n",
        "            answer = \" \".join(answer_words)\n",
        "            tok = tok[:]\n",
        "            [tok.remove(x) for x in subject + VP + prep_part_tokens + answer_words + [rem_verb_phrase] if x in tok ]\n",
        "            quest_tok = prep_part_tokens + [\"how\",\"much\"]+ [verb] + subject + VP + tok + [\"?\"]\n",
        "            #print(quest_tok)\n",
        "            question = \" \".join(quest_tok)\n",
        "            QA.append({\"Q\" : question , \"A\" : answer })\n",
        "            \n",
        "    return QA\n",
        "\n",
        "def QSG_Rule_6_2(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases):\n",
        "    QA = []\n",
        "    for chunk,parse_tree,is_cl,ner,tok,verb in zip(chunks , parse_trees , is_clause_val, ner_split,tokens,verb_phrases) :\n",
        "        rule_grammar = \"{<\\$>*<CD>+<MD>?<VB|VBD|VBG|VBP|VBN|VBZ|IN>+}\"\n",
        "        #print(rule_grammar)\n",
        "        seg_pos = parse_tree.pos()\n",
        "        #print(seg_pos)\n",
        "        rule6_2_chunks = parse_chunks(seg_pos,rule_grammar)\n",
        "        #print(rule6_2_chunks)\n",
        "        rule_chunk = find_chunk(rule6_2_chunks)\n",
        "        #print(rule_chunk)\n",
        "        if rule_chunk :\n",
        "            rule_pos =  [ pos[0] for pos in rule_chunk[0].pos() ]\n",
        "            rule_tokens = [ pos[0][0] for pos in rule_chunk[0].pos() ]\n",
        "            prep_part_tokens = [ p[0] for p in find_chunk(parse_chunks(rule_pos , \"{<\\$>*<CD>+}\" ))[0].leaves()]\n",
        "            #print(prep_part_tokens)\n",
        "            answer_words = prep_part_tokens\n",
        "            [rule_tokens.remove(x) for x in answer_words if x in rule_tokens ]\n",
        "            #calc rest of the sentence for question generation\n",
        "            answer = \" \".join(answer_words)\n",
        "            tok = tok[:]\n",
        "            [tok.remove(x) for x in answer_words + rule_tokens ]\n",
        "            #print(answer)\n",
        "            quest_tok = [\"how\",\"much\"]+ rule_tokens + tok + [\"?\"]\n",
        "            #print(quest_tok)\n",
        "            question = \" \".join(quest_tok)\n",
        "            QA.append({\"Q\" : question , \"A\" : answer })\n",
        "    return QA\n",
        "\n",
        "def QSG_Rule_6_3(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases):\n",
        "    QA = []\n",
        "    for chunk,parse_tree,is_cl,ner,tok,verb in zip(chunks , parse_trees , is_clause_val, ner_split,tokens,verb_phrases) :\n",
        "    # Counter example for is_cl - \"A house is for $100,00\"\n",
        "        rule_grammar = \"{<MD>?<VB|VBD|VBG|VBP|VBN|VBZ>+<IN>?<NN|NNS|NNP|NNPS|PRP|PRP\\$>?<\\$>*<CD>+}\"\n",
        "        #print(rule_grammar)\n",
        "        seg_pos = parse_tree.pos()\n",
        "        #print(seg_pos)\n",
        "        rule6_3_chunks = parse_chunks(seg_pos,rule_grammar)\n",
        "        #print(rule6_1_chunks)\n",
        "        prep_chunk = find_chunk(rule6_3_chunks)\n",
        "        #print(prep_chunk)\n",
        "        if prep_chunk :\n",
        "            prep_pos =  [ pos[0] for pos in prep_chunk[0].pos() ]\n",
        "            #print(prep_pos)\n",
        "            prep_tokens = [ pos[0][0] for pos in prep_chunk[0].pos() ]\n",
        "            ans_tokens = [p[0]for p in find_chunk(parse_chunks(prep_pos , \"{<\\$>*<CD>+}\"))[0].leaves()]\n",
        "            \n",
        "            #print(prep_part_tokens)\n",
        "            VP = find_VP(parse_tree)\n",
        "            rem_verb_phrase =  verb if verb in VP else VP[0]\n",
        "            [prep_tokens.remove(x) for x in ans_tokens + [rem_verb_phrase] if  x in prep_tokens ]\n",
        "            #print(verb)\n",
        "            #print(prep_tokens)\n",
        "            subject = find_subj(parse_tree)\n",
        "            #print(subject)\n",
        "            \n",
        "            #Calc rest of VP for Question Generation\n",
        "            [VP.remove(x) for x in prep_tokens + ans_tokens if x in VP]\n",
        "            #calc rest of the sentence for question generation\n",
        "            answer = \" \".join(ans_tokens)\n",
        "            tok = tok[:]\n",
        "            [tok.remove(x) for x in subject + VP + prep_tokens + ans_tokens + [rem_verb_phrase] if x in tok ]\n",
        "            quest_tok =  [\"how\",\"much\"]+ [verb] + subject + prep_tokens + VP + tok + [\"?\"]\n",
        "            #print(quest_tok)\n",
        "            question = \" \".join(quest_tok)\n",
        "            QA.append({\"Q\" : question , \"A\" : answer })\n",
        "    return QA\n",
        "                \n",
        "                \n",
        "\n",
        "#print(QSG_Rule_6_1(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))\n",
        "#print(QSG_Rule_6_2(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))\n",
        "#print(QSG_Rule_6_3(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7KbD8I5WSiD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_ner_tags_for_pos (ners,pos):\n",
        "    return list(filter(lambda x : x[0] in [p[0] for p in pos] , ners))\n",
        "\n",
        "def QSG_Rule_1(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases):\n",
        "    QA = []\n",
        "    for chunk,parse_tree,is_cl,ner,tok in zip(chunks , parse_trees , is_clause_val, ner_split,tokens) :\n",
        "        if is_cl :\n",
        "            chunk_pos = [ pos[0] for pos in find_chunk(chunk)[0].pos() ]\n",
        "            grammar = \"{<DT>?<JJ.?>*<NN.?|PRP|PRP$|POS|IN|DT|CC|VBG|VBN>+}\"\n",
        "            rule1_chunks = parse_chunks(chunk_pos,grammar)\n",
        "            noun_chunk = find_chunk(rule1_chunks)\n",
        "            if noun_chunk :\n",
        "                noun_pos = noun_chunk[0].leaves()\n",
        "                #print(ner)\n",
        "                #print(noun_pos)\n",
        "                tok = tok[:]\n",
        "                ner_tags = find_ner_tags_for_pos(ner,noun_pos)\n",
        "                qsd4,q_disambg = QSD_Rule_4(ner_tags,\"QSG_RULE_1\")\n",
        "                answer_words = [ p[0] for p in noun_pos ]\n",
        "                #print(tok)\n",
        "                #print(answer_words)\n",
        "                [ tok.remove(ans) for ans in answer_words if ans in tok]\n",
        "                answer = \" \".join(answer_words)\n",
        "                #print(tok)\n",
        "                quest_tok = [q_disambg] + tok + [\"?\"]\n",
        "                #print(quest_tok)\n",
        "                question = \" \".join(quest_tok)\n",
        "                QA.append({\"Q\" : question , \"A\" : answer })\n",
        "    return QA\n",
        "            \n",
        "#QSG_Rule_1(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases)            \n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OQghn5xWSiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def QSG_Rule_7(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases):\n",
        "    QA = []\n",
        "    for chunk,parse_tree,is_cl,ner,tok,verb in zip(chunks , parse_trees , is_clause_val, ner_split,tokens,verb_phrases) :\n",
        "    # Counter example for is_cl - \"A house is for $100,00\"\n",
        "        rule_grammar = \"{<DT>?<CD>+<RB>?<JJ|JJR|JJS>?<NN|NNS|NNP|NNPS|VBG>+}\"\n",
        "        #print(rule_grammar)\n",
        "        seg_pos = parse_tree.pos()\n",
        "        #print(seg_pos)\n",
        "        rule7_chunks = parse_chunks(seg_pos,rule_grammar)\n",
        "        #print(rule7_chunks)\n",
        "        prep_chunk = find_chunk(rule7_chunks)\n",
        "        #print(prep_chunk)\n",
        "        if prep_chunk :\n",
        "            prep_pos =  [ pos[0] for pos in prep_chunk[0].pos() ]\n",
        "            #print(prep_pos)\n",
        "            prep_tokens = [ pos[0][0] for pos in prep_chunk[0].pos() ]\n",
        "            ans_tokens = [p[0]for p in find_chunk(parse_chunks(prep_pos , \"{<CD>+}\"))[0].leaves()]\n",
        "            \n",
        "            #print(prep_part_tokens)\n",
        "            VP = find_VP(parse_tree)\n",
        "            if not VP :\n",
        "                break\n",
        "            rem_verb_phrase =  verb if verb in VP else VP[0]\n",
        "            [prep_tokens.remove(x) for x in ans_tokens if  x in prep_tokens ]\n",
        "            #print(verb)\n",
        "            #print(prep_tokens)\n",
        "            subject = find_subj(parse_tree)\n",
        "            #print(subject)\n",
        "            \n",
        "            #Calc rest of VP for Question Generation\n",
        "            [VP.remove(x) for x in prep_tokens + ans_tokens + [rem_verb_phrase] if x in VP]\n",
        "            #calc rest of the sentence for question generation\n",
        "            answer = \" \".join(ans_tokens)\n",
        "            tok = tok[:]\n",
        "            [tok.remove(x) for x in subject + VP + prep_tokens + ans_tokens + [rem_verb_phrase] if x in tok ]\n",
        "            quest_tok =  [\"how\",\"many\"]+ prep_tokens + [verb] + subject + VP + tok + [\"?\"]\n",
        "            #print(quest_tok)\n",
        "            question = \" \".join(quest_tok)\n",
        "            QA.append({\"Q\" : question , \"A\" : answer })\n",
        "    return QA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3y8uoOLWSiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def QSG_Rule_3(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases):\n",
        "    QA = []\n",
        "    for chunk,parse_tree,is_cl,ner,tok,verb in zip(chunks , parse_trees , is_clause_val, ner_split,tokens,verb_phrases) :\n",
        "    # Counter example for is_cl - \"A house is for $100,00\"\n",
        "        rule_grammar = \"{<PRP\\$|POS>+<RB.?>*<JJ.?>*<NN.?|VBG|VBN>+<VB.?|MD|RP>+}\"\n",
        "        #print(rule_grammar)\n",
        "        seg_pos = parse_tree.pos()\n",
        "        #print(seg_pos)\n",
        "        rule3_chunks = parse_chunks(seg_pos,rule_grammar)\n",
        "        #print(rule3_chunks)\n",
        "        prep_chunk = find_chunk(rule3_chunks)\n",
        "        #print(prep_chunk)\n",
        "        if prep_chunk :\n",
        "            prep_pos =  [ pos[0] for pos in prep_chunk[0].pos() ]\n",
        "            #print(prep_pos)\n",
        "            prep_tokens = [ pos[0][0] for pos in prep_chunk[0].pos() ]\n",
        "            ans_tokens = [p[0]for p in find_chunk(parse_chunks(prep_pos , \"{<PRP\\$|POS>+}\"))[0].leaves()]\n",
        "            \n",
        "            #print(prep_part_tokens)\n",
        "            [prep_tokens.remove(x) for x in ans_tokens if  x in prep_tokens ]\n",
        "            #print(verb)\n",
        "            #print(prep_tokens)\n",
        "            #subject = find_subj(parse_tree)\n",
        "            #print(subject)\n",
        "            \n",
        "            #Calc rest of VP for Question Generation\n",
        "            #[VP.remove(x) for x in prep_tokens + ans_tokens if x in tok]\n",
        "            #calc rest of the sentence for question generation\n",
        "            answer = \" \".join(ans_tokens)\n",
        "            tok = tok[:]\n",
        "            [tok.remove(x) for x in prep_tokens + ans_tokens  if x in tok ]\n",
        "            quest_tok =  [\"Whose\"]+ prep_tokens + tok + [\"?\"]\n",
        "            #print(quest_tok)\n",
        "            question = \" \".join(quest_tok)\n",
        "            QA.append({\"Q\" : question , \"A\" : answer })\n",
        "    return QA\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0fiQAVZWSic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def QSG_Rule_4(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases):\n",
        "    QA = []\n",
        "    for chunk,parse_tree,is_cl,ner,tok,verb in zip(chunks , parse_trees , is_clause_val, ner_split,tokens,verb_phrases) :\n",
        "    # Counter example for is_cl - \"A house is for $100,00\"\n",
        "        rule_grammar = \"{<DT>?<JJ.?>?<RB>?<IN|TO|RP>+<DT>*<JJ.?>*<NN.?|PP|PRP|PRP\\$ >+<VBG|POS|CD|RB|DT>*}\"\n",
        "        #print(rule_grammar)\n",
        "        seg_pos = parse_tree.pos()\n",
        "        #print(seg_pos)\n",
        "        rule4_chunks = parse_chunks(seg_pos,rule_grammar)\n",
        "        #print(rule4_chunks)\n",
        "        prep_chunk = find_chunk(rule4_chunks)\n",
        "        #print(prep_chunk)\n",
        "        if prep_chunk :\n",
        "            prep_pos =  [ pos[0] for pos in prep_chunk[0].pos() ]\n",
        "            #print(prep_pos)\n",
        "            ans_tokens = [ pos[0][0] for pos in prep_chunk[0].pos() ]\n",
        "            prep_tokens = [p[0]for p in find_chunk(parse_chunks(prep_pos , \"{<IN>+}\"))[0].leaves()] if find_chunk(parse_chunks(prep_pos , \"{<IN>+}\")) else None\n",
        "            #print(prep_tokens)\n",
        "            if not prep_tokens  or  not any([x in [\"under\", \"across\", \"around\", \"along\", \"through\", \"over\", \"into\", \"onto\"] for x in prep_tokens]):\n",
        "                break;\n",
        "            \n",
        "            VP = find_VP(parse_tree)\n",
        "            if not VP :\n",
        "                break\n",
        "            rem_verb_phrase =  verb if verb in VP else VP[0]\n",
        "            \n",
        "            #print(prep_tokens)\n",
        "            #print(VP)\n",
        "            #print(verb)\n",
        "            #print(prep_tokens)\n",
        "            subject = find_subj(parse_tree)\n",
        "            #print(subject)\n",
        "            answer = \" \".join(ans_tokens)\n",
        "            #Calc rest of VP for Question Generation\n",
        "            #[VP.remove(x) for x in ans_tokens + [rem_verb_phrase] if x in VP]\n",
        "            #calc rest of the sentence for question generation\n",
        "            VP = \" \".join(VP).replace(answer , \"\").split(\" \")\n",
        "            VP.remove(rem_verb_phrase)\n",
        "            tok = tok[:]\n",
        "            [tok.remove(x) for x in ans_tokens + subject + VP + [rem_verb_phrase] if x in tok ]\n",
        "            #print(tok)\n",
        "            quest_tok =  [\"Where\"]+ [verb] + subject + VP + tok + [\"?\"]\n",
        "            #print(quest_tok)\n",
        "            question = \" \".join(quest_tok)\n",
        "            QA.append({\"Q\" : question , \"A\" : answer })\n",
        "    return QA\n",
        "\n",
        "#QSG_Rule_4(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to-9-gMEWSil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def QSG_Rule_5(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases):\n",
        "    QA = []\n",
        "    for chunk,parse_tree,is_cl,ner,tok,verb in zip(chunks , parse_trees , is_clause_val, ner_split,tokens,verb_phrases) :\n",
        "    # Counter example for is_cl - \"A house is for $100,00\"\n",
        "        rule_grammar = \"{<DT>?<JJ.?>?<RB>?<IN|TO|RP>+<DT>*<NN.?>+}\"\n",
        "        #print(rule_grammar)\n",
        "        seg_pos = parse_tree.pos()\n",
        "        #print(seg_pos)\n",
        "        rule5_chunks = parse_chunks(seg_pos,rule_grammar)\n",
        "        #print(rule5_chunks)\n",
        "        prep_chunk = find_chunk(rule5_chunks)\n",
        "        #print(prep_chunk)\n",
        "        if prep_chunk :\n",
        "            prep_pos =  [ pos[0] for pos in prep_chunk[0].pos() ]\n",
        "            #print(prep_pos)\n",
        "            prep_tokens = [ pos[0][0] for pos in prep_chunk[0].pos() ]\n",
        "            ans_pos = [p for p in find_chunk(parse_chunks(prep_pos , \"{<IN|TO|RP>+<DT>*<NN.?>+}\"))[0].leaves()]\n",
        "            ans_tokens = [ pos[0] for pos in ans_pos ]\n",
        "            [prep_tokens.remove(x) for x in ans_tokens]\n",
        "            #print(prep_tokens)\n",
        "            ans_ner = [ p[1] for p in find_ner_tags_for_pos(ner,ans_pos) ]\n",
        "            not_date_time_ner = not(\"DATE\" in ans_ner or \"TIME\" in ans_ner )\n",
        "            #print(ans_ner)\n",
        "            if not_date_time_ner and (not any([x.lower() in [\"tomorrow\",\"yesterday\", \"today\", \"tonight\", \"am\", \"pm\"] for x in ans_tokens])):\n",
        "                break;\n",
        "            \n",
        "            VP = find_VP(parse_tree)\n",
        "            if not VP :\n",
        "                break\n",
        "            rem_verb_phrase =  verb if verb in VP else VP[0]\n",
        "            \n",
        "            #print(prep_tokens)\n",
        "            #print(VP)\n",
        "            #print(verb)\n",
        "            #print(prep_tokens)\n",
        "            subject = find_subj(parse_tree)\n",
        "            #print(subject)\n",
        "            answer = \" \".join(ans_tokens)\n",
        "            #Calc rest of VP for Question Generation\n",
        "            #[VP.remove(x) for x in ans_tokens + [rem_verb_phrase] if x in VP]\n",
        "            #calc rest of the sentence for question generation\n",
        "            VP = \" \".join(VP).replace(answer , \"\").split(\" \")\n",
        "            VP.remove(rem_verb_phrase)\n",
        "            [VP.remove(x) for x in prep_tokens ]\n",
        "            tok = tok[:]\n",
        "            [tok.remove(x) for x in ans_tokens + subject + VP + [rem_verb_phrase] + prep_tokens if x in tok ]\n",
        "            #print(tok)\n",
        "            quest_tok =  [\"When\"]+ [verb] + subject + VP + prep_tokens + tok + [\"?\"]\n",
        "            #print(quest_tok)\n",
        "            question = \" \".join(quest_tok)\n",
        "            QA.append({\"Q\" : question , \"A\" : answer })\n",
        "    return QA\n",
        "\n",
        "#QSG_Rule_5(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "te-JY6wZWSip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def QSG_Rule_2_4(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases):\n",
        "    QA = []\n",
        "    for chunk,parse_tree,is_cl,ner,tok,verb in zip(chunks , parse_trees , is_clause_val, ner_split,tokens,verb_phrases) :\n",
        "    # Counter example for is_cl - \"A house is for $100,00\"\n",
        "        rule_grammar = \"{<TO>+<VB|VBP|RP>+<DT>?<RB.?>*<JJ.?>*<NN.?|PRP|PRP\\$|POS|VBG|DT>*}\"\n",
        "        #print(rule_grammar)\n",
        "        seg_pos = parse_tree.pos()\n",
        "        #print(seg_pos)\n",
        "        rule2_4_chunks = parse_chunks(seg_pos,rule_grammar)\n",
        "        #print(rule2_4_chunks)\n",
        "        prep_chunk = find_chunk(rule2_4_chunks)\n",
        "        #print(prep_chunk)\n",
        "        if prep_chunk :\n",
        "            prep_pos =  [ pos[0] for pos in prep_chunk[0].pos() ]\n",
        "            #print(prep_pos)\n",
        "            prep_tokens = [ pos[0][0] for pos in prep_chunk[0].pos() ]\n",
        "            ans_chunk = find_chunk(parse_chunks(prep_pos , \"{<DT>?<RB.?>*<JJ.?>*<NN.?|PRP|PRP\\$|POS|VBG|DT>*}\"))\n",
        "            ans_NP = None\n",
        "            if ans_chunk :\n",
        "                ans_NP = True\n",
        "                rep_chunk = find_chunk(parse_chunks(prep_pos , \"{<TO>+<VB|VBP|RP>+}\"))\n",
        "                \n",
        "            else :\n",
        "                ans_chunk = find_chunk(parse_chunks(prep_pos , \"{<TO>+<VB|VBP|RP>+}\"))\n",
        "                ans_NP = False\n",
        "            \n",
        "            ans_pos = [p for p in ans_chunk[0].leaves()]\n",
        "            ans_tokens = [ pos[0] for pos in ans_pos ]\n",
        "            #print(ans_pos)\n",
        "            #print(ans_tokens)\n",
        "            if ans_NP :\n",
        "                rep_pos = [p for p in rep_chunk[0].leaves()]\n",
        "                rep_tokens = [ pos[0] for pos in rep_pos ]\n",
        "            else:\n",
        "                rep_tokens = [\"to\" , \"do\"]\n",
        "            \n",
        "            prep = \" \".join(prep_tokens)\n",
        "            rep = \" \".join(rep_tokens)\n",
        "            #print(prep)\n",
        "            \n",
        "            #print(rep)\n",
        "            rem_VP = rep\n",
        "            #print(rem_VP)\n",
        "            #print(rep_tokens)\n",
        "            #rem_VP = \" \".join(prep_tokens).replace(\" \".join(ans_tokens),\" \".join(rep_tokens))\n",
        "            #print(rem_VP)\n",
        "            [prep_tokens.remove(x) for x in ans_tokens]\n",
        "            #print(prep_tokens)\n",
        "            #ans_ner = [ p[1] for p in find_ner_tags_for_pos(ner,ans_pos) ]\n",
        "            #not_date_time_ner = not(\"DATE\" in ans_ner or \"TIME\" in ans_ner )\n",
        "            #print(ans_ner)\n",
        "            #if not_date_time_ner and (not any([x.lower() in [\"tomorrow\",\"yesterday\", \"today\", \"tonight\", \"am\", \"pm\"] for x in ans_tokens])):\n",
        "                #break;\n",
        "            \n",
        "            VP = find_VP(parse_tree)\n",
        "            if not VP :\n",
        "                break\n",
        "            \n",
        "            #print(prep_tokens)\n",
        "            #print(VP)\n",
        "            #print(verb)\n",
        "            #print(prep_tokens)\n",
        "            subject = find_subj(parse_tree)\n",
        "            #print(subject)\n",
        "            answer = \" \".join(ans_tokens)\n",
        "            #Calc rest of VP for Question Generation\n",
        "            [VP.remove(x) for x in ans_tokens + rem_VP.split(\" \") if x in VP]\n",
        "            #calc rest of the sentence for question generation\n",
        "            #VP = \" \".join(VP).replace(answer , \"\").split(\" \")\n",
        "            #VP.remove(rem_verb_phrase)\n",
        "            #[VP.remove(x) for x in prep_tokens ]\n",
        "            tok = tok[:]\n",
        "            [tok.remove(x) for x in ans_tokens + subject + VP + rem_VP.split(\" \") + ans_tokens if x in tok ]\n",
        "            #print(tok)\n",
        "            quest_tok =  [\"What\"]+ [verb] + subject + VP + rem_VP.split(\" \") + tok + [\"?\"]\n",
        "            #print(quest_tok)\n",
        "            question = \" \".join(quest_tok)\n",
        "            QA.append({\"Q\" : question , \"A\" : answer })\n",
        "    return QA\n",
        "\n",
        "#QSG_Rule_2_4(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9f4oRLeWSiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Bringing here the implementation of QSD rule befor implementing QSG 2.1 - 2.4 \n",
        "def find_ner_tag_for_token(ners,token):\n",
        "    for tag in ner :\n",
        "        if tag[0] == token :\n",
        "            return tag[1]\n",
        "    return None\n",
        "\n",
        "def find_ner_tag_for_tokens(ners,tokens):\n",
        "    return list(filter(lambda x : x[0] in tokens , ners))\n",
        "\n",
        "def get_pos_tokens_from_chunk_tree(chunk_tree):\n",
        "    rep_pos = [p for p in chunk_tree.leaves()] if chunk_tree else []\n",
        "    rep_tokens = [ pos[0] for pos in rep_pos ] if chunk_tree else []\n",
        "    return rep_pos,rep_tokens\n",
        "def QSD_Rule_1(chunk_pos) :\n",
        "    #print(ner_chunk_tags)\n",
        "    disambg_value =  all( [x == \"PRP\" for x in [p[1] for p in chunk_pos]])\n",
        "    if disambg_value :\n",
        "        return disambg_value,\"whom\"\n",
        "    else:\n",
        "        return disambg_value,\"what\"\n",
        "def QSD_Rule_2(parse_tree):\n",
        "    VP = None\n",
        "    #parse_tree.draw()\n",
        "    for child  in parse_tree :\n",
        "        #print (child.label())\n",
        "        if child.label() == \"VP\":\n",
        "            VP =  child\n",
        "    if VP :\n",
        "        NP_in_VP = []\n",
        "        for child  in VP :\n",
        "            if child.label() == \"NP\":\n",
        "                NP_in_VP.append(child)\n",
        "        #print(NP_in_VP)\n",
        "        if len(NP_in_VP) > 1 :\n",
        "            return True , NP_in_VP[0].leaves() , NP_in_VP[1].leaves()\n",
        "        else :\n",
        "            return False , [] , NP_in_VP[0].leaves() if NP_in_VP else []\n",
        "    else :\n",
        "        return False , [] , []\n",
        "\n",
        "def QSD_Rule_3(chunk_pos,chunk_ners) :\n",
        "    first_noun_chunk  = find_chunk(parse_chunks(chunk_pos , \"{<NN.?>+}\"))\n",
        "    if first_noun_chunk :\n",
        "        first_noun_pos, first_noun_tokens = get_pos_tokens_from_chunk_tree(first_noun_chunk[0])\n",
        "        if find_ner_tag_for_token(chunk_ners,first_noun_tokens[0]) == \"PERSON\":\n",
        "            return True,\"Whom\"\n",
        "        else:\n",
        "            return False,\"What\"\n",
        "    else: \n",
        "        return False,\"What\"\n",
        "\n",
        "    \n",
        "        \n",
        "def QSD_Rule_4(ner_chunk_tags,QSG_rule) :\n",
        "    #print(ner_chunk_tags)\n",
        "    disambg_value =  ner_chunk_tags[0][1] in ['LOCATION','ORGANIZATION', 'CITY','COUNTRY']\n",
        "    if QSG_rule == \"QSG_RULE_1\" :\n",
        "        if disambg_value :\n",
        "            return disambg_value,\"what\"\n",
        "        elif ner_chunk_tags[0][1] in ['PERSON']:\n",
        "            return disambg_value,\"who\"\n",
        "        else:\n",
        "            return disambg_value,\"who\"\n",
        "    if QSG_rule == \"QSG_RULE_2_1\":\n",
        "        if disambg_value :\n",
        "            return disambg_value,\"where\"\n",
        "        else :\n",
        "            return disambg_value,\"To what\"\n",
        "    if QSG_rule == \"QSG_RULE_2_2\":\n",
        "        if disambg_value :\n",
        "            return disambg_value,\"where\"\n",
        "        else :\n",
        "            return disambg_value,\"what\"\n",
        "\n",
        "def QSD_Rule_5(chunk_pos,chunk_ners) :\n",
        "    noun_chunk  = find_chunk(parse_chunks(chunk_pos , \"{<NN.?>+}\"))\n",
        "    if noun_chunk :\n",
        "        noun_pos, noun_tokens = get_pos_tokens_from_chunk_tree(noun_chunk[0])\n",
        "        noun_ners = find_ner_tag_for_tokens(chunk_ners,noun_tokens)\n",
        "        ners = set([ x[1] for x in noun_ners])\n",
        "        if \"TIME\" in ners or \"DATE\" in ners :\n",
        "            return \"when\"\n",
        "        else:\n",
        "            return \"what\"\n",
        "    else: \n",
        "        return \"What\"\n",
        "\n",
        "    \n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2tyBVn-WSix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def QSG_Rule_2_1(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases):\n",
        "    QA = []\n",
        "    for chunk,parse_tree,is_cl,ner,tok,verb in zip(chunks , parse_trees , is_clause_val, ner_split,tokens,verb_phrases) :\n",
        "    # Counter example for is_cl - \"A house is for $100,00\"\n",
        "        rule_grammar = \"{<TO>+<DT>?<RB.?>*<JJ.?>*<NN.?|PRP|PRP\\$|VBG|DT|POS|CD|VBN>+}\"\n",
        "        seg_pos = parse_tree.pos()\n",
        "        rule2_1_chunks = parse_chunks(seg_pos,rule_grammar)\n",
        "        seg = \" \".join([p[0] for p in seg_pos ])\n",
        "        #print(seg)\n",
        "        #print(rule2_1_chunks)\n",
        "        prep_chunk = find_chunk(rule2_1_chunks)\n",
        "        prep_pos,prep_tokens = get_pos_tokens_from_chunk_tree(prep_chunk[0]) if prep_chunk else (None,None)\n",
        "        #print(prep_pos)\n",
        "        #print(prep_tokens)\n",
        "        prep = \" \".join(prep_tokens) if prep_tokens else []\n",
        "        clause_chunk  = find_chunk(chunk)\n",
        "        #print(clause_chunk)\n",
        "        if len(clause_chunk) > 1 :\n",
        "            clause_strings = [\" \".join(get_pos_tokens_from_chunk_tree(c)[1]) for c in clause_chunk ]\n",
        "            prep_index = seg.index(seg)\n",
        "            clause_index  = [abs(seg.index(cs)-prep_index)  for cs in clause_strings ]\n",
        "            cl_chunk = clause_chunk[clause_index.index(min(clause_index))]\n",
        "            cl_string = \" \".split(clause_strings[clause_index.index(min(clause_index))])\n",
        "            #print(cl_string)\n",
        "            verb = verb_phrase_identification([findLCA(parse_tree,cl_string[0],cl_string[-1])],[True],[cl_chunk])\n",
        "            #print(verb)\n",
        "        else :\n",
        "            cl_chunk = clause_chunk[0] if clause_chunk else None\n",
        "        cl_pos,cl_tokens = get_pos_tokens_from_chunk_tree(cl_chunk)\n",
        "        #print(seg_pos)\n",
        "        #print(prep_chunk)\n",
        "        #print(cl_chunk)\n",
        "        if prep_chunk :\n",
        "            \n",
        "            ques = \"To what\"\n",
        "            qsd1,ques = QSD_Rule_1(prep_pos)\n",
        "            if qsd1 :\n",
        "                ques = \"To \" + ques\n",
        "            else :\n",
        "                prep_ners = find_ner_tag_for_tokens(ner,prep_tokens)\n",
        "                qsd3,ques = QSD_Rule_3(prep_pos,prep)\n",
        "                ques = \"To \" + ques\n",
        "                if not qsd3 :\n",
        "                    qsd4,ques = QSD_Rule_4(prep_ners,\"QSG_RULE_2_1\")\n",
        "                \n",
        "            #print(ques)\n",
        "            #print(verb)\n",
        "                \n",
        "            VP = find_VP(parse_tree)\n",
        "            #print(parse_tree)\n",
        "            if not VP :\n",
        "                break\n",
        "            rem_verb_phrase =  verb if verb in VP else VP[0]\n",
        "            #print(VP)\n",
        "            #print(verb)\n",
        "            \n",
        "            #print(prep_part_tokens)\n",
        "            #[prep_tokens.remove(x) for x in ans_tokens if  x in prep_tokens ]\n",
        "            #print(verb)\n",
        "            #print(prep_tokens)\n",
        "            subject = find_subj(parse_tree)\n",
        "            #print(subject)\n",
        "            \n",
        "            #Calc rest of VP for Question Generation\n",
        "            [VP.remove(x) for x in prep_tokens + [rem_verb_phrase]  if x in tok]\n",
        "            #calc rest of the sentence for question generation\n",
        "            #answer = \" \".join(ans_tokens)\n",
        "            tok = tok[:]\n",
        "            [tok.remove(x) for x in prep_tokens + VP + [rem_verb_phrase] + subject   if x in tok ]\n",
        "            quest_tok =  [ques]+ [verb] + subject + VP  + tok + [\"?\"]\n",
        "            #print(quest_tok)\n",
        "            question = \" \".join(quest_tok)\n",
        "            QA.append({\"Q\" : question , \"A\" : prep })\n",
        "    return QA\n",
        "\n",
        "#QSG_Rule_2_1(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R_SCuv7WSi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def QSG_Rule_2_2(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases):\n",
        "    QA = []\n",
        "    for chunk,parse_tree,is_cl,ner,tok,verb in zip(chunks , parse_trees , is_clause_val, ner_split,tokens,verb_phrases) :\n",
        "    # Counter example for is_cl - \"A house is for $100,00\"\n",
        "        rule_grammar = \"{<IN>+<DT>?<RB.?>*<JJ.?>*<NN.?|PRP|PRP\\$|POS|VBG|DT|CD|VBN>+}\"\n",
        "        seg_pos = parse_tree.pos()\n",
        "        rule2_2_chunks = parse_chunks(seg_pos,rule_grammar)\n",
        "        seg = \" \".join([p[0] for p in seg_pos ])\n",
        "        #print(seg)\n",
        "        #print(rule2_2_chunks)\n",
        "        prep_chunk = find_chunk(rule2_2_chunks)\n",
        "        prep_pos,prep_tokens = get_pos_tokens_from_chunk_tree(prep_chunk[0]) if prep_chunk else (None,None)\n",
        "        #print(prep_pos)\n",
        "        #print(prep_tokens)\n",
        "        prep = \" \".join(prep_tokens) if prep_tokens else []\n",
        "        clause_chunk  = find_chunk(chunk)\n",
        "        \n",
        "        if len(clause_chunk) > 1 :\n",
        "            clause_strings = [\" \".join(get_pos_tokens_from_chunk_tree(c)[1]) for c in clause_chunk ]\n",
        "            prep_index = seg.index(seg)\n",
        "            clause_index  = [abs(seg.index(cs)-prep_index)  for cs in clause_strings ]\n",
        "            cl_chunk = clause_chunk[clause_index.index(min(clause_index))]\n",
        "            cl_string = clause_strings[clause_index.index(min(clause_index))].split(\" \")\n",
        "            verb = verb_phrase_identification([findLCA(parse_tree,cl_string[0],cl_string[-1])],[True],[cl_chunk])[0]\n",
        "        else :\n",
        "            cl_chunk = clause_chunk[0] if clause_chunk else None\n",
        "        cl_pos,cl_tokens = get_pos_tokens_from_chunk_tree(cl_chunk)\n",
        "        #print(cl_pos)\n",
        "        #print(cl_tokens)\n",
        "        if prep_chunk :\n",
        "            q_prep = find_chunk(parse_chunks(seg_pos,\"{<IN+>}\"))[0]\n",
        "            q_prep_pos,q_prep_tokens = get_pos_tokens_from_chunk_tree(q_prep)\n",
        "            #print(q_prep_tokens)\n",
        "            ques = \"what\"\n",
        "            qsd1,ques = QSD_Rule_1(prep_pos)\n",
        "            if not qsd1 :\n",
        "                prep_ners = find_ner_tag_for_tokens(ner,prep_tokens)\n",
        "                qsd3,ques = QSD_Rule_3(prep_pos,prep_ners)\n",
        "                ques = ques\n",
        "                if not qsd3 :\n",
        "                    qsd4,ques = QSD_Rule_4(prep_ners,\"QSG_RULE_2_2\")\n",
        "                if not qsd4 :\n",
        "                    ques = QSD_Rule_5(prep_pos,prep_ners)\n",
        "            #print(ques)\n",
        "                \n",
        "            VP = find_VP(parse_tree)\n",
        "            if not VP :\n",
        "                break\n",
        "            rem_verb_phrase =  verb if verb in VP else VP[0]\n",
        "            #print(VP)\n",
        "            #print(verb)\n",
        "            \n",
        "            #print(prep_part_tokens)\n",
        "            #[prep_tokens.remove(x) for x in ans_tokens if  x in prep_tokens ]\n",
        "            #print(verb)\n",
        "            #print(prep_tokens)\n",
        "            subject = find_subj(parse_tree)\n",
        "            #print(subject)\n",
        "            \n",
        "            #Calc rest of VP for Question Generation\n",
        "            [VP.remove(x) for x in prep_tokens + [rem_verb_phrase]  if x in tok]\n",
        "            #calc rest of the sentence for question generation\n",
        "            #answer = \" \".join(ans_tokens)\n",
        "            tok = tok[:]\n",
        "            [tok.remove(x) for x in prep_tokens + VP + [rem_verb_phrase] + subject   if x in tok ]\n",
        "            quest_tok = q_prep_tokens +[ques]+ [verb] + subject + VP  + tok + [\"?\"]\n",
        "            #print(quest_tok)\n",
        "            question = \" \".join(quest_tok)\n",
        "            QA.append({\"Q\" : question , \"A\" : prep })\n",
        "    return QA\n",
        "\n",
        "#QSG_Rule_2_2(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUd9SYDqWSi5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def QSG_Rule_2_3(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases):\n",
        "    QA = []\n",
        "    for chunk,parse_tree,is_cl,ner,tok,verb in zip(chunks , parse_trees , is_clause_val, ner_split,tokens,verb_phrases) :\n",
        "    # Counter example for is_cl - \"A house is for $100,00\"\n",
        "        rule_grammar = \"{<VB.?|MD|RP|RB.?>+<DT>?<RB.?>*<JJ.?>*<NN.?|PRP|PRP\\$|POS|VBG|DT|CD|VBN>+}\"\n",
        "        seg_pos = parse_tree.pos()\n",
        "        rule2_3_chunks = parse_chunks(seg_pos,rule_grammar)\n",
        "        seg = \" \".join([p[0] for p in seg_pos ])\n",
        "        #print(seg)\n",
        "        #print(rule2_3_chunks)\n",
        "        prep_chunk = find_chunk(rule2_3_chunks)\n",
        "        prep_pos,prep_tokens = get_pos_tokens_from_chunk_tree(prep_chunk[0]) if prep_chunk else (None,None)\n",
        "        #print(prep_pos)\n",
        "        #print(prep_tokens)\n",
        "        prep = \" \".join(prep_tokens) if prep_tokens else []\n",
        "        \n",
        "        #print(cl_pos)\n",
        "        #print(cl_tokens)\n",
        "        if prep_chunk :\n",
        "            ques = \"what\"\n",
        "            two_ques = False\n",
        "            qsd1,ques = QSD_Rule_1(prep_pos)\n",
        "            prep_ners = find_ner_tag_for_tokens(ner,prep_tokens)\n",
        "            qsd2 , prp_tokens , noun_tokens = QSD_Rule_2(parse_tree)\n",
        "            two_ques = qsd2\n",
        "            if not qsd2 :\n",
        "                qsd3,ques = QSD_Rule_3(prep_pos,prep_ners)\n",
        "            #print(ques)\n",
        "            VP = find_VP(parse_tree)\n",
        "            if not VP :\n",
        "                break\n",
        "            #print(VP)\n",
        "            rem_verb_phrase =  verb if verb in VP else VP[0]\n",
        "            \n",
        "            #print(verb)\n",
        "            subject = find_subj(parse_tree)\n",
        "            #print(subject)\n",
        "            ans_tokens = noun_tokens\n",
        "            if two_ques :\n",
        "                ques1_ans = prp_tokens\n",
        "                VP_q1 = VP[:]\n",
        "                tok_q1 = tok[:]\n",
        "                [VP_q1.remove(x) for x in ques1_ans  if x in VP_q1]\n",
        "                [tok_q1.remove(x) for x in ques1_ans + VP_q1 + [rem_verb_phrase] + subject   if x in tok_q1 ]\n",
        "                q1_token = [\"Whom\"]+ [verb] + subject + VP_q1  + tok_q1 + [\"?\"]\n",
        "                ques1 = \" \".join(q1_token)\n",
        "                QA.append({\"Q\" : ques1 , \"A\" : \" \".join(ques1_ans)  })\n",
        "           \n",
        "            \n",
        "            \n",
        "            #print(prep_part_tokens)\n",
        "            #[prep_tokens.remove(x) for x in ans_tokens if  x in prep_tokens ]\n",
        "            #print(verb)\n",
        "            #print(prep_tokens)\n",
        "            \n",
        "            \n",
        "            \n",
        "            #Calc rest of VP for Question Generation\n",
        "            [VP.remove(x) for x in ans_tokens  if x in VP]\n",
        "            #calc rest of the sentence for question generation\n",
        "            answer = \" \".join(ans_tokens)\n",
        "            tok = tok[:]\n",
        "            [tok.remove(x) for x in ans_tokens + VP + [rem_verb_phrase] + subject   if x in tok ]\n",
        "            quest_tok = [ques]+ [verb] + subject + VP  + tok + [\"?\"]\n",
        "            #print(quest_tok)\n",
        "            question = \" \".join(quest_tok)\n",
        "            QA.append({\"Q\" : question , \"A\" : answer  })\n",
        "    return QA\n",
        "\n",
        "#QSG_Rule_2_3(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wU71dEsWSi9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = \"Barack Obama is the president of The United States of America.\"\n",
        "#sentence = \"The boy went by bus.\"\n",
        "#sentence =  sentence.rstrip().rstrip(\".\")\n",
        "\n",
        "#sentence = \" Deputy Chief Minister and Services Department Minister-in-charge Manish Sisodia ordered the Department to change the approving authority for transfers of officers from the Lieutenant-Governor and bureaucrats to the Chief Minister and Ministers.\"\n",
        "#sentence = \"The contractor will build you a house for $100,000 dollars.\"\n",
        "#sentence = \"The book might cost me $10.\"\n",
        "#sentence=\"The book might cost me $10 from the store.\"\n",
        "#sentence = \"$100,000 builds a house out of sticks.\"\n",
        "#sentence = \"The bill will cost them 500 million dollars in India.\"\n",
        "\n",
        "#sentence = \"His name is Robinson.\"\n",
        "#sentence = \"She will quickly pour the sticky liquid into the green flowery pot.\"\n",
        "#sentence = \"I am going quickly back on Saturday.\"\n",
        "#sentence = \"He wants to become a good doctor.\"\n",
        "#sentence = \"I want to work.\"\n",
        "#sentence = \"He hurriedly left the class in the morning.\"\n",
        "#sentence = \"He is addicted to smoking.\"\n",
        "#sentence = \"He will go by bus.\"\n",
        "#sentence = \"John gave Mary a book.\" #design more rules to catch the essence\n",
        "#sentence = \"He gave him a book.\"\n",
        "#sentence = \"He will buy a book.\"\n",
        "#sentence = \"He gave him a book.\"\n",
        "#sentence = \"John gave Mary a book.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0_5JrC_WSjD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#All preprocessing\n",
        "segments = sentence.rstrip().rstrip(\".\").split(\", \")\n",
        "tree = Tree.fromstring(nlp.parse(sentence))\n",
        "ner = nlp.ner(sentence)\n",
        "tokens = [word_tokenize(segment) for segment in segments]\n",
        "parse_trees = [findLCA(tree,seg[0],seg[-1]) for seg in tokens]\n",
        "ner_split = [list(g) for k,g in itertools.groupby(ner,lambda x: x[0] == ',') if not k]\n",
        "clause_identification_grammar = \"{<DT>?<JJ.?>*<\\$|CD|NN.?|PRP|PRP\\$|POS|IN|DT|CC|VBG|VBN>+<RB.?|VB.?|MD|RP>+}\"\n",
        "chunks = [ parse_chunks(pos_tag(word_tokenize(segment)) ,clause_identification_grammar) for segment in segments ]\n",
        "is_clause_val = is_clause(chunks)\n",
        "\n",
        "new_trees,enrichment_update =  enrich_VPs(parse_trees)\n",
        "parse_trees = new_trees\n",
        "update_indices = indices = [i for i, x in enumerate(enrichment_update) if x == True]\n",
        "\n",
        "for index in update_indices : \n",
        "    tree = parse_trees[index]\n",
        "    #Not changing original segments as they will be used later to form questions\n",
        "    #segments[index] =  \" \".join(tree.leaves())\n",
        "    chunks[index] = parse_chunks(tree.pos() ,clause_identification_grammar)\n",
        "    ner_split[index]  = nlp.ner(\" \".join(tree.leaves()))\n",
        "    tokens[index] = tree.leaves()\n",
        "    #print(ner_split)\n",
        "    #print(tree.pos())\n",
        "\n",
        "verb_phrases = verb_phrase_identification(parse_trees,is_clause_val,chunks)\n",
        "    #all the pre processed data we have\n",
        "#print(segments)\n",
        "#print(chunks)\n",
        "#print(parse_trees)\n",
        "#for ptree in parse_trees :\n",
        "    #ptree.draw()\n",
        "#print(verb_phrases)\n",
        "#print(is_clause_val)\n",
        "#print(tokens)\n",
        "#print(ner_split)\n",
        "                "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "wu38HZ0jWSjL",
        "colab_type": "code",
        "outputId": "d32dcd5b-e331-449e-a0de-31dbcdb42aac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "print(QSG_Rule_1(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))\n",
        "print(QSG_Rule_2_1(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))\n",
        "print(QSG_Rule_2_2(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))\n",
        "print(QSG_Rule_2_3(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))\n",
        "print(QSG_Rule_2_4(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))\n",
        "print(QSG_Rule_3(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))\n",
        "print(QSG_Rule_4(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))\n",
        "print(QSG_Rule_5(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))\n",
        "print(QSG_Rule_6_1(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))\n",
        "print(QSG_Rule_6_2(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))\n",
        "print(QSG_Rule_6_3(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))\n",
        "print(QSG_Rule_7(chunks , parse_trees , is_clause_val, ner_split, tokens,verb_phrases))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'Q': 'who is the president of The United States of America ?', 'A': 'Barack Obama'}]\n",
            "[]\n",
            "[{'Q': 'of what does Barack Obama the president States of America ?', 'A': 'of The United'}]\n",
            "[{'Q': 'What does Barack Obama is ?', 'A': 'the president of The United States of America'}]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6l-rMrLWSjQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}